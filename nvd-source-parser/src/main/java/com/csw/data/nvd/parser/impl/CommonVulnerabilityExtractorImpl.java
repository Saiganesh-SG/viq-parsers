package com.csw.data.nvd.parser.impl;

import java.io.IOException;
import java.sql.Timestamp;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.google.gson.Gson;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

import com.csw.data.nvd.audit.NvdJobStatusEnumeration;
import com.csw.data.nvd.audit.NvdParserAudit;
import com.csw.data.nvd.audit.RecordDetails;
import com.csw.data.nvd.config.ParseType;
import com.csw.data.nvd.json.cpe.source.DefCpeMatch;
import com.csw.data.nvd.json.cpedictionary.targets.CpeDictionary;
import com.csw.data.nvd.json.targets.VendorComment;
import com.csw.data.nvd.json.targets.Vulnerability;
import com.csw.data.nvd.parser.CommonVulnerabilityExtractor;
import com.csw.data.nvd.parser.CveProcessor;
import com.csw.data.nvd.parser.TopicProcessor;
import com.csw.data.nvd.service.LiveKeepService;
import com.csw.data.util.CommonUtils;
import com.csw.data.util.ParserConstants;
import com.csw.data.util.ParserFileUtils;
import com.fasterxml.jackson.databind.ObjectMapper;

@Service
@Qualifier("CommonVulnerabilityExtractor")
public class CommonVulnerabilityExtractorImpl implements CommonVulnerabilityExtractor {

    private static final Logger LOGGER = LoggerFactory.getLogger(CommonVulnerabilityExtractorImpl.class);

    @Value("#{'${parser.cve.download.latest.url}'.split(',')}")
    private List<String> cveSourceDirectoryLatest;

    @Value("#{'${parser.cve.download.url}'.split(',')}")
    private List<String> cveDownloadUrls;

    @Value("${parser.vendorcomments.download.url}")
    private List<String> vendorCommentUrls;
    
    @Value("#{'${parser.cpe.download.url}'.split(',')}")
    private List<String> cpeDownloadUrls;
    
    @Value("${parser.cpe.dictionary.download.url}")
    private List<String> cpeDictionaryDownloadUrls;

    @Value("${parser.cve.source.directory}")
    private String cveSourceDirectory;

    @Value("${parser.cve.local.directory}")
    private String cveLocalDirectory;
    
    @Value("${parser.cpe.source.directory}")
    private String cpeSourceDirectory;
    
    @Autowired
    private CveProcessor cveProcessor;
    
    @Autowired
    private TopicProcessor<DefCpeMatch> cpeProcessor;
    
    @Autowired
    private TopicProcessor<CpeDictionary> cpeDictionaryProcessor;

    @Autowired
    private LiveKeepService<Vulnerability> liveKeepService;
    
    @Autowired
    private LiveKeepService<DefCpeMatch> cpeLiveKeepServiceImpl;

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    @Value("${data.kafka.vulnerability.topic}")
    private String vulnerabilityKafkaTopic;
    
    @Value("${data.kafka.product.topic}")
    private String productKafkaTopic;
    
    @Value("${data.kafka.cpe.dictionary.topic}")
    private String cpeDictionaryKafkaTopic;

    @Autowired
    Gson gson;
    
    @Override
    public void parseCve(boolean processLatest) throws IOException {
        LOGGER.info("started parsing CVE..");
        
        // download and extract cve source files and process every files
        List<String> cveSourceUrl = getSourceUrl(processLatest);
        List<String> sourceFiles = ParserFileUtils.extractSourceFilesWithExtension(cveSourceDirectory, cveSourceUrl, ParseType.CVE.name(), ParserConstants.JSON_FILE_EXTENSION);
        LOGGER.info("sourceFiles : {}", sourceFiles);
        
        // extract vendor comments from NVD
        Map<String, List<VendorComment>> vendorComments = cveProcessor.extractVendorComments(vendorCommentUrls);
        
        for (String sourceFile : sourceFiles) {
            // record the parser start time
            LocalDateTime startTime = LocalDateTime.now();
            
            // process vulnerabilities from the source file
            List<Vulnerability> vulnerabilities = cveProcessor.extractVulnerabilitiesFromSource(sourceFile, vendorComments);

            // write the file to livekeep and return kafka message
            Map<String, Integer> recordStats = initializeRecordStatMap();
            JSONArray kafkaMessage = liveKeepService.writeFileToLiveKeep(vulnerabilities, cveLocalDirectory, recordStats);
            List<JSONArray> kafkaMessagePartioned = CommonUtils.splitJsonArrayByChunkLimit(kafkaMessage, 1000);
            LOGGER.info("sourceFile : {}", sourceFile);
            LOGGER.info("kafkaMessage size : {}", kafkaMessage.length());
            LOGGER.info("kafkaMessagePartioned size : {}", kafkaMessagePartioned.size());
            
            // push the message to kafka topic
            for (JSONArray message : kafkaMessagePartioned) {
                try {
                    JSONObject updatedMessage = updateKafkaMessage(message);
                    kafkaTemplate.send(vulnerabilityKafkaTopic, updatedMessage.toString());
                }
                catch (Exception e) {
                    LOGGER.info("Error while sending the message");
                }
            }
            
            //update the final job audit
            LocalDateTime endTime = LocalDateTime.now();
            NvdParserAudit audit = logAudit(vulnerabilities.size(), startTime, endTime, recordStats, ParseType.CVE.toString());
            ObjectMapper logMapper = new ObjectMapper();
            String jobAudit = logMapper.writeValueAsString(audit);
            LOGGER.info("Job Audit : {}", jobAudit);
        }
        LOGGER.info("CVE data process completed..");
    }
    
    @Override
    public void parseCpe() throws IOException {

        Integer chunkSize = 5;

        LOGGER.info("Total Memory = {}", Runtime.getRuntime().totalMemory());
        LOGGER.info("Max Memory = {}", Runtime.getRuntime().maxMemory());
        LOGGER.info("Free Memory = {}", Runtime.getRuntime().freeMemory());
        
        // download and extract cve source files and process every files
        List<String> cpeSourceFiles = ParserFileUtils.extractSourceFilesWithExtension(cpeSourceDirectory, cpeDownloadUrls, ParseType.CPE.name(), ParserConstants.JSON_FILE_EXTENSION);
        
        for (String cpeSourceFile : cpeSourceFiles) {
            
            var startTime = LocalDateTime.now();
            
            //unmarshall the cpeSourceFile
            List<DefCpeMatch> defCpeMatchs = cpeProcessor.unmarshallObjectFromSourceFile(cpeSourceFile);
            
            //write the cpe file and meta file to livekeep and create kafka message
            Map<String, Integer> recordStats = initializeRecordStatMap();
            JSONArray kafkaMessage = cpeLiveKeepServiceImpl.writeFileToLiveKeep(defCpeMatchs, null, recordStats);
            List<JSONArray> kafkaMessagePartioned = CommonUtils.splitJsonArrayByChunkLimit(kafkaMessage, chunkSize);
            Integer msgCount = 0;
            Integer batchCount = 0;
            LOGGER.info("Total Batch Size = {} | Msg(s) per Batch = {}", kafkaMessagePartioned.size(), chunkSize);
            //publish the kafka message
            for (JSONArray message : kafkaMessagePartioned) {
                try {
                    JSONObject updatedMessage = updateKafkaMessage(message);
                    String kafkaMsg = updatedMessage.toString();
                    kafkaTemplate.send(productKafkaTopic, kafkaMsg).get();
                    msgCount = msgCount + message.length();
                    batchCount = batchCount + 1;
                    LOGGER.info("Kafka Msg - Batch No {} Sent | Total Msg(s) Sent {}", batchCount, msgCount);
                }
                catch (Exception e) {
                    LOGGER.info("Error while sending the message - {}", e.getMessage());
                }
            }
            //audit logs or this job
            LocalDateTime endTime = LocalDateTime.now();
            NvdParserAudit audit = logAudit(defCpeMatchs.size(), startTime, endTime, recordStats, ParseType.CPE.toString());
            var logMapper = new ObjectMapper();
            var jobAudit = logMapper.writeValueAsString(audit);
            LOGGER.info("Job Audit : {}", jobAudit);
        }
    }

    private String convertToString(JSONObject jsonObject) throws JsonProcessingException {
        String output = null;
        Map<String, Object> jsonMap = jsonObject.toMap();
        ObjectMapper mapper = new ObjectMapper();
        output = mapper.writeValueAsString(jsonMap);
        //output = gson.toJson(jsonMap);
        return output;
    }
    
    @Override
    public void parseCpeDictionary() throws IOException {
        
        // download and extract cpe dictionary source files and process every files
        List<String> cpeDictionarySourceFiles = ParserFileUtils.extractSourceFilesWithExtension(cpeSourceDirectory, cpeDictionaryDownloadUrls, ParseType.CPE_DICTIONARY.name(), ParserConstants.XML_FILE_EXTENSION);
        
        for (String cpeDictionarySourceFile : cpeDictionarySourceFiles) {
            var startTime = LocalDateTime.now();
            
            //unmarshall the cpeDictionarySourceFile
            List<CpeDictionary> cpeDictionaryList = cpeDictionaryProcessor.unmarshallObjectFromSourceFile(cpeDictionarySourceFile);
            LOGGER.info("cpeDictionary size : {}", cpeDictionaryList.size());
            
            //write the cpe file and meta file to livekeep and create kafka message
            Map<String, Integer> recordStats = initializeRecordStatMap();
            
            JSONArray kafkaMessage = cpeLiveKeepServiceImpl.writeCpeDictionaryFileToKafka(cpeDictionaryList, null, recordStats);
            LOGGER.info("kafkaMessage size : {}", kafkaMessage.length());
            List<JSONArray> kafkaMessagePartioned = CommonUtils.splitJsonArrayByChunkLimit(kafkaMessage, 25);
            
            //publish the kafka message
            for (JSONArray message : kafkaMessagePartioned) {
                try {
                    JSONObject updatedMessage = updateKafkaMessage(message);
                    kafkaTemplate.send(cpeDictionaryKafkaTopic, updatedMessage.toString()).get();
                }
                catch (Exception e) {
                    LOGGER.info("Error while sending the message");
                }
            }
            
            
            //audit logs or this job
            LocalDateTime endTime = LocalDateTime.now();
            NvdParserAudit audit = logAudit(cpeDictionaryList.size(), startTime, endTime, recordStats, ParseType.CPE_DICTIONARY.toString());
            var logMapper = new ObjectMapper();
            var jobAudit = logMapper.writeValueAsString(audit);
            LOGGER.info("Job Audit : {}", jobAudit);
            
        }
    }
    
    /**
     * Update kafka message.
     *
     * @param kafkaMessage the kafka message
     * @return the JSON object
     * @throws JSONException the JSON exception
     */
    private JSONObject updateKafkaMessage(JSONArray kafkaMessage) throws JSONException {
        var jsonObject = new JSONObject();
        jsonObject.put("forceUpdate", "true");
        jsonObject.put("messages", kafkaMessage);
        return jsonObject;
    }
    
    private List<String> getSourceUrl(boolean processLatest) {
        if (processLatest) {
            return cveSourceDirectoryLatest;
        }
        else {
            return cveDownloadUrls;
        }
    }
    
    private NvdParserAudit logAudit(int totalRecords, LocalDateTime startTime, LocalDateTime endTime, Map<String, Integer> recordStats, String parserType) {
        NvdParserAudit audit = new NvdParserAudit();
        DateTimeFormatter auditTimeFormat = DateTimeFormatter.ofPattern("uuuu/MM/dd HH:mm:ss");
        Long jobStartTime = TimeUnit.MILLISECONDS.toSeconds(Timestamp.valueOf(startTime).getTime()); 
        Long jobEndTime = TimeUnit.MILLISECONDS.toSeconds(Timestamp.valueOf(endTime).getTime());

        RecordDetails recordDetails = new RecordDetails();
        recordDetails.setTotalRecords(totalRecords);
        recordDetails.setNewRecords(recordStats.get("newRecords"));
        recordDetails.setModifiedRecords(recordStats.get("modifiedRecords"));
        
        String jobName = "NVD Parser";
        switch (parserType) {
            case "CVE":
                jobName = "NVD CVE Parser";
                break;
            case "CPE":
                jobName = "NVD CPE Parser";
                break;        
            default:
                break;
        }
        audit.setJobName(jobName);
        audit.setRefreshType("Full Refresh");
        audit.setStartTime(auditTimeFormat.format(startTime));
        audit.setEndTime(auditTimeFormat.format(endTime));
        audit.setTotalTime(String.valueOf(jobEndTime - jobStartTime));
        audit.setRecordDetails(recordDetails);
        audit.setJobStatus(NvdJobStatusEnumeration.COMPLETED);
        return audit;
    }

    /**
     * Initialize record stat map.
     *
     * @return the map
     */
    private Map<String, Integer> initializeRecordStatMap() {
        Map<String, Integer> recordsStatMap = new HashMap<>();
        recordsStatMap.put("newRecords", 0);
        recordsStatMap.put("modifiedRecords", 0);
        recordsStatMap.put("failedRecords", 0);
        return recordsStatMap;
    }

}
